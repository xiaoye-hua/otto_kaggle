{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "In this notebook we will train a `Word2Vec` model.\n",
    "\n",
    "We will use the `gensim` library which offers extremely fast training on the CPU.\n",
    "\n",
    "We will again rely on `polars` and its small memory footprint to load and process the data. To speed things up, let's use the dataset in a parquet format (we won't have to deal with `jasonl` files anymore). [I shared the dataset here](https://www.kaggle.com/datasets/radek1/otto-full-optimized-memory-footprint). \n",
    "\n",
    "Why are we training word2vec embeddings in the first place?\n",
    "\n",
    "A session where one action follows another action is very much like... a sentence! In sentences, words that are related appear together. We don't necessarily expect to see the word 'spaceship' in a sentence discussing various ways to cook a steak. The word \"steak\" is more likely to appear close to words such as rosemary, salt, pepper, oil and butter. In this sense, these words are thematically related. And with a large enough corpus we can start making further distinctions! Maybe butter will appear closer in the embedding space to milk than for instance to orange juice, even though both are drinks you can have with your breakfast! (that might be due to milk having the property of being a substance used to produce butter, which might tip the embeddings for \"milk\" and \"butter\" closer together assuming our corpus would contain texts on butter production!).\n",
    "\n",
    "Similarly here we can exploit the fact that `aids` appearing in a sequence close together likely share some similarity. A person browsing for gardening equipment is probably not looking at surfboards and vice versa.\n",
    "\n",
    "Once we train our model, what will we be able to use it for? First and foremost, candidate generation! Though one might also imagine using it for scoring. Essentially, a model such as this can be very handy in the context of session-based recommendation models!\n",
    "\n",
    "Let's get to work! üôÇ\n",
    "\n",
    "## Other resources you might find useful:\n",
    "\n",
    "* [üí° [2 methods] How-to ensemble predictions üèÖüèÖüèÖ](https://www.kaggle.com/code/radek1/2-methods-how-to-ensemble-predictions)\n",
    "* [co-visitation matrix - simplified, imprvd logic üî•](https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic)\n",
    "* [üí° Word2Vec How-to [training and submission]üöÄüöÄüöÄ](https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission)\n",
    "* [local validation tracks public LB perfecty -- here is the setup](https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991)\n",
    "* [üí° For my friends from Twitter and LinkedIn -- here is how to dive into this competition üê≥](https://www.kaggle.com/competitions/otto-recommender-system/discussion/368560)\n",
    "* [Full dataset processed to CSV/parquet files with optimized memory footprint](https://www.kaggle.com/competitions/otto-recommender-system/discussion/363843)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:49:33.350660Z",
     "iopub.status.busy": "2023-01-15T06:49:33.350272Z",
     "iopub.status.idle": "2023-01-15T06:49:33.355564Z",
     "shell.execute_reply": "2023-01-15T06:49:33.354490Z",
     "shell.execute_reply.started": "2023-01-15T06:49:33.350619Z"
    }
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "debug_rows = 10000\n",
    "\n",
    "vector_size = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ../model_training/w2v_v1: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir ../model_training/w2v_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:47:11.366026Z",
     "iopub.status.busy": "2023-01-15T06:47:11.365433Z",
     "iopub.status.idle": "2023-01-15T06:47:52.940058Z",
     "shell.execute_reply": "2023-01-15T06:47:52.939279Z",
     "shell.execute_reply.started": "2023-01-15T06:47:11.365944Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install polars\n",
    "\n",
    "import polars as pl\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_dir = '../data/parquet/train1/*.parquet'\n",
    "test_dir = '../data/parquet/train2/*.parquet'\n",
    "\n",
    "\n",
    "model_file = '../model_training/w2v_v1/w2v.model'\n",
    "if debug:\n",
    "    train = pl.read_parquet(train_dir, n_rows=debug_rows)\n",
    "    test = pl.read_parquet(test_dir, n_rows=debug_rows)\n",
    "else:\n",
    "    train = pl.read_parquet(train_dir)\n",
    "    test = pl.read_parquet(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now transform the data into a format that the `gensim` library can work with. Thanks to `polars` we can do so very efficiently and very quickly.\n",
    "\n",
    "There are various ways we could feed our data to our model, however doing so straight from RAM in the form of Python lists is probably one of the fastest! As we have enough resources on Kaggle to do so, let us take this approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:48:13.691282Z",
     "iopub.status.busy": "2023-01-15T06:48:13.690745Z",
     "iopub.status.idle": "2023-01-15T06:48:13.698261Z",
     "shell.execute_reply": "2023-01-15T06:48:13.697310Z",
     "shell.execute_reply.started": "2023-01-15T06:48:13.691254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107685893, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:48:16.440654Z",
     "iopub.status.busy": "2023-01-15T06:48:16.439909Z",
     "iopub.status.idle": "2023-01-15T06:48:16.446487Z",
     "shell.execute_reply": "2023-01-15T06:48:16.445595Z",
     "shell.execute_reply.started": "2023-01-15T06:48:16.440609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9580522, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:48:19.018121Z",
     "iopub.status.busy": "2023-01-15T06:48:19.017708Z",
     "iopub.status.idle": "2023-01-15T06:48:27.678248Z",
     "shell.execute_reply": "2023-01-15T06:48:27.677094Z",
     "shell.execute_reply.started": "2023-01-15T06:48:19.018090Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences_df = pl.concat([train, test]).groupby('session').agg(\n",
    "    pl.col('aid').alias('sentence')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:49:07.623658Z",
     "iopub.status.busy": "2023-01-15T06:49:07.623242Z",
     "iopub.status.idle": "2023-01-15T06:49:07.630252Z",
     "shell.execute_reply": "2023-01-15T06:49:07.629163Z",
     "shell.execute_reply.started": "2023-01-15T06:49:07.623624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10005085, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:49:53.935731Z",
     "iopub.status.busy": "2023-01-15T06:49:53.935325Z",
     "iopub.status.idle": "2023-01-15T06:49:53.950970Z",
     "shell.execute_reply": "2023-01-15T06:49:53.949946Z",
     "shell.execute_reply.started": "2023-01-15T06:49:53.935693Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = sentences_df['sentence'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T06:52:12.172052Z",
     "iopub.status.busy": "2023-01-15T06:52:12.171684Z",
     "iopub.status.idle": "2023-01-15T06:52:12.177075Z",
     "shell.execute_reply": "2023-01-15T06:52:12.175787Z",
     "shell.execute_reply.started": "2023-01-15T06:52:12.172023Z"
    }
   },
   "outputs": [],
   "source": [
    "# set([1]) + set([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T07:00:20.793271Z",
     "iopub.status.busy": "2023-01-15T07:00:20.791804Z",
     "iopub.status.idle": "2023-01-15T07:00:22.786119Z",
     "shell.execute_reply": "2023-01-15T07:00:22.784924Z",
     "shell.execute_reply.started": "2023-01-15T07:00:20.793226Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "w2vec = Word2Vec(sentences=sentences, vector_size=vector_size, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.wv[16246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model.wv[16246]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}